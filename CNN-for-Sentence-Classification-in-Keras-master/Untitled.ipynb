{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model variation is CNN-rand\n",
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import data_helpers\n",
    "from w2v import train_word2vec\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Input, Merge, Convolution1D, MaxPooling1D\n",
    "\n",
    "np.random.seed(2)\n",
    "\n",
    "# Parameters\n",
    "# ==================================================\n",
    "#\n",
    "# Model Variations. See Kim Yoon's Convolutional Neural Networks for\n",
    "# Sentence Classification, Section 3 for detail.\n",
    "\n",
    "\n",
    "model_variation = 'CNN-rand'  # CNN-rand | CNN-non-static | CNN-static\n",
    "print('Model variation is %s' % model_variation)\n",
    "\n",
    "# Model Hyperparameters\n",
    "sequence_length = 125\n",
    "embedding_dim = 20\n",
    "filter_sizes = (3, 4)\n",
    "num_filters = 150\n",
    "dropout_prob = (0.25, 0.5)\n",
    "hidden_dims = 150\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 32\n",
    "num_epochs = 1\n",
    "val_split = 0.2\n",
    "\n",
    "# Word2Vec parameters, see train_word2vec\n",
    "min_word_count = 1  # Minimum word count\n",
    "context = 10        # Context window size\n",
    "\n",
    "# Data Preparatopn\n",
    "# ==================================================\n",
    "#\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "x, y, vocabulary, vocabulary_inv = data_helpers.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(117811, 125) [[0 0 0 ..., 0 1 0]\n",
      " [0 0 0 ..., 1 0 0]\n",
      " [0 0 0 ..., 1 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 1 0]\n",
      " [0 0 0 ..., 0 0 1]\n",
      " [0 0 0 ..., 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print x.shape, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 53530\n",
      "(117811, 125) [  212  1714  1001    71  6677  3798    13    49   269    90     3  2025\n",
      "    32    15   201 18300   100    16  2544   211   141    24    16  2725\n",
      "   755  1232   244    10    43    19     7    60    31     1   119   168\n",
      "     7    74    23     4 11661   244    10    94   488   322   166     2\n",
      "   285   120   166     9   979   148    10  1592    12   624     2    17\n",
      "   833  3361   240     9  1184     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0]\n",
      "(117811,) 10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if model_variation == 'CNN-non-static' or model_variation == 'CNN-static':\n",
    "    embedding_weights = train_word2vec(\n",
    "        x, vocabulary_inv, embedding_dim, min_word_count, context)\n",
    "    if model_variation == 'CNN-static':\n",
    "        x = embedding_weights[0][x]\n",
    "elif model_variation == 'CNN-rand':\n",
    "    embedding_weights = None\n",
    "else:\n",
    "    raise ValueError('Unknown model variation')\n",
    "\n",
    "# Shuffle data\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = x[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices].argmax(axis=1)\n",
    "\n",
    "print(\"Vocabulary Size: {:d}\".format(len(vocabulary)))\n",
    "\n",
    "# Building model\n",
    "# ==================================================\n",
    "#\n",
    "# graph subnet with one input and one output,\n",
    "# convolutional layers concateneted in parallel\n",
    "graph_in = Input(shape=(sequence_length, embedding_dim))\n",
    "convs = []\n",
    "for fsz in filter_sizes:\n",
    "    conv = Convolution1D(nb_filter=num_filters,\n",
    "                         filter_length=fsz,\n",
    "                         border_mode='valid',\n",
    "                         activation='relu',\n",
    "                         subsample_length=1)(graph_in)\n",
    "    pool = MaxPooling1D(pool_length=2)(conv)\n",
    "    flatten = Flatten()(pool)\n",
    "    convs.append(flatten)\n",
    "\n",
    "if len(filter_sizes) > 1:\n",
    "    out = Merge(mode='concat')(convs)\n",
    "else:\n",
    "    out = convs[0]\n",
    "\n",
    "graph = Model(input=graph_in, output=out)\n",
    "\n",
    "# main sequential model\n",
    "model = Sequential()\n",
    "if not model_variation == 'CNN-static':\n",
    "    model.add(Embedding(len(vocabulary), embedding_dim, input_length=sequence_length,\n",
    "                        weights=embedding_weights))\n",
    "model.add(Dropout(dropout_prob[0], input_shape=(\n",
    "    sequence_length, embedding_dim)))\n",
    "model.add(graph)\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(dropout_prob[1]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "print x_shuffled.shape, x_shuffled[1]\n",
    "print y_shuffled.shape, y_shuffled[1]\n",
    "# Training model\n",
    "# ==================================================\n",
    "# model.fit(x_shuffled, y_shuffled, batch_size=batch_size,\n",
    "#           nb_epoch=num_epochs, validation_split=val_split, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(69356, 125) (69356, 11)\n",
      "Vocabulary Size: 41279\n",
      "[   9  149  271  714   31   21    6    2  135    1 2319  455   10   71    3\n",
      "   52  138 1719    5    4   38   35   32    9   69  162   56 1589    3  229\n",
      "   44  268   12  143  798   16   15   56 1044   26    9  141   41    1  130\n",
      "    2  271  743   18    1  172  349   94  150    1  120  573  149   58   27\n",
      "    1   79   57   98  158    3   25   95   43   10   16    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0] [35540 62156 49799 ..., 59148 40106  9173]\n",
      "(69356, 125) [   13    15    59    10     1    72   259    26   118    45    12     1\n",
      "  5121   895    77     6   133     4    50    79  4842    82    46     2\n",
      "    13   446     7    51     7   152     3    95    18   145   323   635\n",
      "   120     1 25538   253   103    13     5   283     1   101    38   442\n",
      "    35    12     1   895     2   126     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0]\n",
      "(69356,) 0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'predict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-cbc639797464>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0my_shuffled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_shuffled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# predict = model.predict_classes(x, batch_size=batch_size,verbose=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'predict' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.feature_extraction.text import CountVectorizer,HashingVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 5.34270836687\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x_shuffled, y_shuffled)\n",
    "\n",
    "# Make a pipeline to do unigrams then run linear regression\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
    "pipeline = Pipeline([\n",
    "        ('regression', RandomForestRegressor(n_estimators=200,max_depth=25,n_jobs=-1))\n",
    "])\n",
    "cv = GridSearchCV(\n",
    "    pipeline, {}\n",
    ").fit(X_train, y_train)\n",
    "mse = mean_squared_error(y_test, cv.predict(X_test))\n",
    "print(\"MSE: {}\".format(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(117811, 125) (117811, 11)\n"
     ]
    }
   ],
   "source": [
    "x_, y_, vocabulary_, vocabulary_inv_ = data_helpers.load_pre_data()\n",
    "print x.shape, y.shape\n",
    "predictions = cv.predict(x_)\n",
    "y_quality = [0 for i in range(len(predictions))]\n",
    "outfile = open('predict.out','wb')\n",
    "for i in range(len(predictions)):\n",
    "    y_quality[i] = predictions[i]\n",
    "    outfile.write(str(y_quality[i]) + '\\n')\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
