{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.feature_extraction.text import CountVectorizer,HashingVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "np.random.seed(1337)\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Embedding\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, SimpleRNN, GRU\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read train data.\n",
    "rawdata = pd.read_csv('/home/lich/Workspace/Rate-My-Professor/Data/train.csv')\n",
    "rawdata['comments'] = rawdata['comments'].fillna('')\n",
    "comment = [x.lower() for x in rawdata['comments']]\n",
    "quality = rawdata['quality']\n",
    "# Read test data\n",
    "rawdata = pd.read_csv('/home/lich/Workspace/Rate-My-Professor/Data/test.csv')\n",
    "rawdata['comments'] = rawdata['comments'].fillna('')\n",
    "rawcomment_test = [x.lower() for x in rawdata['comments']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Configuration for Word\n",
    "BASE_DIR = '/home/lich/Workspace/Learning/NLP_Learning/lstm'\n",
    "GLOVE_DIR = BASE_DIR + '/glove.6B/'\n",
    "TEXT_DATA_DIR = BASE_DIR + '/20_newsgroup/'\n",
    "MAX_SEQUENCE_LENGTH = 200\n",
    "MAX_NB_WORDS = 187200\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "batch_size = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# first, build index mapping words in the embeddings set\n",
    "# to their embedding vector\n",
    "\n",
    "print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text dataset\n",
      "Found 117811 texts for training.\n",
      "Found 69356 tests for testing.\n",
      "Found 187167 texts in all.\n",
      "Found 117811 labels.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# second, prepare text samples and their labels\n",
    "# get train and test text\n",
    "print('Processing text dataset')\n",
    "\n",
    "texts = comment  # list of text samples\n",
    "labels_index = {}  # dictionary mapping label name to numeric id\n",
    "labels = quality  # list of label ids\n",
    "\n",
    "test_texts = rawcomment_test\n",
    "# for name in sorted(os.listdir(TEXT_DATA_DIR)):\n",
    "#     path = os.path.join(TEXT_DATA_DIR, name)\n",
    "#     if os.path.isdir(path):\n",
    "#         label_id = len(labels_index)\n",
    "#         labels_index[name] = label_id\n",
    "#         for fname in sorted(os.listdir(path)):\n",
    "#             if fname.isdigit():\n",
    "#                 fpath = os.path.join(path, fname)\n",
    "#                 if sys.version_info < (3,):\n",
    "#                     f = open(fpath)\n",
    "#                 else:\n",
    "#                     f = open(fpath, encoding='latin-1')\n",
    "#                 texts.append(f.read())\n",
    "#                 f.close()\n",
    "#                 labels.append(label_id)\n",
    "\n",
    "print('Found %s texts for training.' % len(texts))\n",
    "print('Found %s tests for testing.' % len(test_texts))\n",
    "len_train = len(texts)\n",
    "len_test = len(test_texts)\n",
    "texts = texts + test_texts\n",
    "print('Found %s texts in all.' % len(texts))\n",
    "print('Found %s labels.' % len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 73633 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# finally, vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "# get test text samples into a 2D interger tensor\n",
    "# test_tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "# test_tokenizer.fit_on_texts(test_texts)\n",
    "# test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
    "\n",
    "# test_word_index = tokenizer.word_index\n",
    "# print('Found %s unique tokens.' % len(test_word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30, 38, 55, 2219, 169, 20, 6, 961, 46, 1, 151, 288, 212, 147, 180, 20, 6, 2, 9, 58, 265, 387, 4, 1115, 24, 757, 5, 1, 71, 10, 1, 4405, 9, 186, 238, 165, 3, 77, 2, 347, 24, 1, 65, 16, 42, 140, 42, 15, 198, 34, 3, 427, 1526, 32, 15, 19, 1, 179, 68, 650, 1, 753, 2, 117, 21, 7, 156, 1, 4]\n",
      "178\n",
      "124\n"
     ]
    }
   ],
   "source": [
    "print(sequences[0])\n",
    "print(word_index[\"love\"])\n",
    "print(np.max(map(lambda x: len(x.split()), comment)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (187167, 200)\n",
      "Shape of label tensor: (117811,)\n",
      "Preparing embedding matrix.\n",
      "Shape of train: (94249, 200) (94249, 11)\n",
      "Shape of validate: (23562, 200) (23562, 11)\n",
      "Shape of test: (69356, 200)\n"
     ]
    }
   ],
   "source": [
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "train_data = data[:len_train]\n",
    "\n",
    "_labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(train_data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "train_data = train_data[indices]\n",
    "_labels = _labels[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * train_data.shape[0])\n",
    "\n",
    "# nb_validation_samples = 20000\n",
    "x_train = train_data[:-nb_validation_samples]\n",
    "y_train = _labels[:-nb_validation_samples]\n",
    "x_val = train_data[-nb_validation_samples:]\n",
    "y_val = _labels[-nb_validation_samples:]\n",
    "x_test = data[len_train:]\n",
    "\n",
    "print('Preparing embedding matrix.')\n",
    "print('Shape of train:', x_train.shape, y_train.shape)\n",
    "print('Shape of validate:', x_val.shape, y_val.shape)\n",
    "print('Shape of test:', x_test.shape)\n",
    "# print(y_train[:4])\n",
    "# print(labels[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23562\n"
     ]
    }
   ],
   "source": [
    "print(nb_validation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(73634, 100)\n"
     ]
    }
   ],
   "source": [
    "# prepare embedding matrix\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print(embedding_matrix.shape)\n",
    "\n",
    "# for test embedding matrix\n",
    "# test_nb_words = min(MAX_NB_WORDS, len(test_word_index))\n",
    "# test_embedding_matrix = np.zeros((test_nb_words + 1, EMBEDDING_DIM))\n",
    "# for word, i in test_word_index.items():\n",
    "#     if i > MAX_NB_WORDS:\n",
    "#         continue\n",
    "#     embedding_vector = embeddings_index.get(word)\n",
    "#     if embedding_vector is not None:\n",
    "#         # words not found in embedding index will be all-zeros.\n",
    "#         test_embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# print(test_embedding_matrix.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients.py:90: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 94249 samples, validate on 23562 samples\n",
      "Epoch 1/5\n",
      "94249/94249 [==============================] - 2180s - loss: 0.2672 - acc: 0.9091 - val_loss: 0.2510 - val_acc: 0.9091\n",
      "Epoch 2/5\n",
      "94249/94249 [==============================] - 2325s - loss: 0.2432 - acc: 0.9125 - val_loss: 0.2327 - val_acc: 0.9159\n",
      "Epoch 3/5\n",
      "94249/94249 [==============================] - 2379s - loss: 0.2322 - acc: 0.9154 - val_loss: 0.2255 - val_acc: 0.9159\n",
      "Epoch 4/5\n",
      "94249/94249 [==============================] - 2377s - loss: 0.2264 - acc: 0.9163 - val_loss: 0.2216 - val_acc: 0.9167\n",
      "Epoch 5/5\n",
      "94249/94249 [==============================] - 2460s - loss: 0.2219 - acc: 0.9175 - val_loss: 0.2193 - val_acc: 0.9170\n",
      "23562/23562 [==============================] - 165s   \n",
      "Test score: 0.219270950718\n",
      "Test accuracy: 0.916988835651\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = Embedding(nb_words + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "#                             trainable=False,\n",
    "                            dropout=0.2)\n",
    "batch_size = 32\n",
    "\n",
    "print('Build model...')\n",
    "# sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "# embedded_sequences = embedding_layer(sequence_input)\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(LSTM(100, dropout_W=0.2, dropout_U=0.2))  # try using a GRU instead, for fun\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(11, activation='softmax'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train, batch_size=batch_size, nb_epoch=5,\n",
    "          validation_data=(x_val, y_val))\n",
    "score, acc = model.evaluate(x_val, y_val,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69356/69356 [==============================] - 485s   \n",
      "(69356,)\n"
     ]
    }
   ],
   "source": [
    "predict = model.predict_classes(x_test, batch_size=32, verbose=1)\n",
    "print(predict.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2 10 10 ..., 10  2  2]\n",
      "[[ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  1. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  1. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  1.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "print(predict)\n",
    "print(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23562/23562 [==============================] - 162s   \n"
     ]
    }
   ],
   "source": [
    "re = model.predict_classes(x_val, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10 10  2 10 10  2  2 10 10 10]\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "print(re[:10])\n",
    "print(len(y_val[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "submit = pd.read_csv('submission1.csv')\n",
    "submit['quality'] = predict\n",
    "submit.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
